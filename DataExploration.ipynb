{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf18674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Evaluation of Sinusitis Surgery Recommendation\n",
    "\n",
    "# This script evaluates LLM clinical decision-making for sinusitis surgery.\n",
    "# The workflow is as follows:\n",
    "# 1.  Setup: Load libraries and configure API keys.\n",
    "# 2.  Data Loading: Load and combine multiple years of patient data from BigQuery.\n",
    "# 3.  Preprocessing: \n",
    "    # - Group all records by patient ID.\n",
    "    # - For each patient, create a sorted, longitudinal clinical note history.\n",
    "    # - Identify the date of surgery (if any) and exclude pre-operative/post-operative notes.\n",
    "    # - Censor any sentences mentioning surgical plans to create a \"blinded\" note for the LLM.\n",
    "    # - Aggregate all other clinical data (labs, meds, demographics).\n",
    "    # - Create a clean, flat DataFrame where each row is a unique patient.\n",
    "# 4.  LLM Analysis:\n",
    "#     - Generate a structured prompt for each case.\n",
    "#     - Send the prompt to the GPT-4 API and parse the JSON response.\n",
    "# 5.  Evaluation:\n",
    "#     - Compare the LLM's decision against actual surgery CPT codes.\n",
    "#     - Calculate accuracy, precision, recall, and F1-score.\n",
    "#     - Analyze the LLM's confidence on correct vs. incorrect predictions.\n",
    "# 6.  Output: Save the full results and a sample for human evaluation to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c78e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import openai\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# For progress bars\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "# For BigQuery access\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa43b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (\n",
    "    censor_surgical_plans,\n",
    "    extract_ent_notes,\n",
    "    check_list_for_keywords,\n",
    "    process_procedures,\n",
    "    extract_radiology_report,\n",
    "    query_openai,\n",
    "    generate_prompt,\n",
    "    preprocess_all_patients\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd6e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI API Key\n",
    "try:\n",
    "    with open(\"openai_key.txt\", \"r\") as f:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = f.read().strip()\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "except FileNotFoundError:\n",
    "    print(\"OpenAI key file not found. Make sure 'openai_key.txt' is in the directory,\")\n",
    "    # Exit if key is not found, as the script cannot proceed.\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf26b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading - Read in datasets from BigQuery\n",
    "\n",
    "print(\"Connecting to BigQuery to load datasets...\")\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "\n",
    "# Create dictionary to hold dataframes\n",
    "dataframes = {}\n",
    "\n",
    "for table_name in DATA_TABLES:\n",
    "     print(f\"Loading and unioning table: '{table_name}'...\")\n",
    "     try:\n",
    "        # Construct a query to UNION this specific table across all yearly datasets\n",
    "        union_query = \"\\nUNION ALL\\n\".join(\n",
    "            [f\"SELECT * FROM `{PROJECT_ID}.{dataset_id}.{table_name}`\" for dataset_id in DATASET_IDS]\n",
    "        )\n",
    "        # Load the data and store it in our dictionary\n",
    "        dataframes[table_name] = client.query(union_query).to_dataframe()\n",
    "        print(f\" -> Loaded {len(dataframes[table_name])} total rows for '{table_name}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- QUERY FAILED for table '{table_name}' ---\")\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(\"Please check that this table exists in all of your yearly datasets.\")\n",
    "        exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52dc8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis - Process data and analyze to LLM\n",
    "for index, case in tqdm(cases_df.iterrows(), total=len(cases_df), desc=\"Evaluating Cases\"):\n",
    "    prompt = generate_prompt(case)\n",
    "    response_text = query_openai(prompt)\n",
    "    processed_data = preprocess_all_patients(case, surgery_codes)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dca9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading data...\")\n",
    "    ...\n",
    "\n",
    "    print(\"Running preprocessing...\")\n",
    "    cases_df = preprocess_all_patients(dataframes, SURGERY_CPT_CODES)\n",
    "\n",
    "    print(\"Running LLM inference...\")\n",
    "    ...\n",
    "\n",
    "    print(\"Evaluating predictions...\")\n",
    "    evaluate_predictions(eval_df)\n",
    "\n",
    "    print(\"Saving results...\")\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d025b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final complete dataframe to a CSV file.\n",
    "full_results_path = \"sinusitis_llm_full_results.csv\"\n",
    "cases_df.to_csv(full_results_path, index=False)\n",
    "print(f\"\\nFull results with all columns saved to '{full_results_path}'\")\n",
    "\n",
    "# Save a sample of 200 cases to a separate CSV for human evaluation.\n",
    "if len(cases_df) > 0:\n",
    "    sample_path = \"sinusitis_llm_human_review_sample.csv\"\n",
    "    # Ensure we don't try to sample more rows than exist\n",
    "    sample_size = min(200, len(cases_df))\n",
    "    cases_df.head(sample_size).to_csv(sample_path, index=False)\n",
    "    print(f\"A sample of {sample_size} cases for human review saved to '{sample_path}'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
