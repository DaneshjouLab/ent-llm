{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf18674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Evaluation of Sinusitis Surgery Recommendation\n",
    "\n",
    "# This script evaluates LLM clinical decision-making for sinusitis surgery.\n",
    "# The workflow is as follows:\n",
    "# 1.  Setup: Load libraries, import preprocessing functions, and configure API keys.\n",
    "# 2.  Preprocessing: \n",
    "    # - Group all records by patient ID.\n",
    "    # - For each patient, create a sorted, longitudinal clinical note history.\n",
    "    # - Identify the date of surgery (if any) and exclude pre-operative/post-operative notes.\n",
    "    # - Censor any sentences mentioning surgical plans to create a \"blinded\" note for the LLM.\n",
    "    # - Aggregate all other clinical data (labs, meds, demographics).\n",
    "    # - Create a clean, flat DataFrame where each row is a unique patient.\n",
    "# 3.  Load in BigQuery data\n",
    "# 4.  LLM Analysis: Send the prompt to the GPT-4 API and parse the JSON response.\n",
    "# 5.  Evaluation:\n",
    "#     - Compare the LLM's decision against actual surgery CPT codes.\n",
    "#     - Calculate accuracy, precision, recall, and F1-score.\n",
    "#     - Analyze the LLM's confidence on correct vs. incorrect predictions.\n",
    "# 6.  Output: Save the full results and a sample for human evaluation to CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c78e9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import openai\n",
    "import re\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# For progress bars\n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "# For BigQuery access\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa43b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import preprocessing functions\n",
    "from utils import (\n",
    "    censor_surgical_plans,\n",
    "    extract_ent_notes,\n",
    "    check_list_for_keywords,\n",
    "    process_procedures,\n",
    "    extract_radiology_report,\n",
    "    query_openai,\n",
    "    generate_prompt,\n",
    "    preprocess_all_patients,\n",
    "    evaluate_predictions\n",
    ")\n",
    "\n",
    "# Import API keys and configuration\n",
    "from config import (\n",
    "    PROJECT_ID,\n",
    "    DATASET_IDS,\n",
    "    DATA_TABLES,\n",
    "    SURGERY_CPT_CODES,\n",
    "    LAB_KEYWORDS,\n",
    "    MED_KEYWORDS,\n",
    "    DIAGNOSTIC_ENDOSCOPY_CPT_CODES\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd6e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI API Key\n",
    "try:\n",
    "    with open(\"/Users/joannalin/Github/ent-llm/data/openai_key.txt\", \"r\") as f:\n",
    "        os.environ[\"OPENAI_API_KEY\"] = f.read().strip()\n",
    "    openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "except FileNotFoundError:\n",
    "    print(\"OpenAI key file not found. Make sure 'openai_key.txt' is in the directory,\")\n",
    "    # Exit if key is not found, as the script cannot proceed.\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf26b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading - Read in datasets from BigQuery\n",
    "\n",
    "print(\"Connecting to BigQuery to load datasets...\")\n",
    "client = bigquery.Client(project=PROJECT_ID)\n",
    "dataframes = {}\n",
    "\n",
    "for table_name in DATA_TABLES:\n",
    "    print(f\"Combining table: '{table_name}'...\")\n",
    "    union_query = \"\\nUNION ALL\\n\".join(\n",
    "        [f\"SELECT * FROM `{PROJECT_ID}.{dataset_id}.{table_name}`\" for dataset_id in DATASET_IDS]\n",
    "    )\n",
    "    full_query = f\"\"\"WITH unioned AS ({union_query}) SELECT * FROM unioned\"\"\"\n",
    "\n",
    "    try:\n",
    "        df = client.query(full_query).to_dataframe()\n",
    "        dataframes[table_name] = df\n",
    "        print(f\"Loaded: {df.shape[0]} rows x {df.shape[1]} columns for '{table_name}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load preview for '{table_name}': {e}\")\n",
    "\n",
    "print(\"\\n=== Loaded Tables Summary ===\")\n",
    "for table_name, df in dataframes.items():\n",
    "    print(f\"\\n--- {table_name.upper()} ---\")\n",
    "    print(\"Columns:\", df.columns.tolist())\n",
    "    print(\"Top 5 rows:\")\n",
    "    print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52dc8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis - Process data\n",
    "print (\"Processing cases for LLM evaluation...\")\n",
    "cases_df = preprocess_all_patients(dataframes, SURGERY_CPT_CODES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24424eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run LLM analysis\n",
    "results = []\n",
    "for _, case in tqdm(cases_df.iterrows(), total=len(cases_df), desc=\"Evaluating Cases\"):\n",
    "    prompt = generate_prompt(case)\n",
    "    response_text = query_openai(prompt)\n",
    "    try:\n",
    "        parsed_response = json.loads(response_text)\n",
    "        results.append({\n",
    "            **case,\n",
    "            'llm_decision': parsed_response.get(\"decision\"),\n",
    "            'llm_confidence': parsed_response.get(\"confidence\"),\n",
    "            'llm_reasoning': parsed_response.get(\"reasoning\")\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse LLM response for case {case['patient_id']}: {e}\")\n",
    "\n",
    "eval_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da30f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate predictions\n",
    "print(\"\\nEvaluating predictions...\")\n",
    "evaluate_predictions(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d025b0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final complete dataframe to a CSV file.\n",
    "print(\"\\nSaving results...\")\n",
    "full_results_path = \"sinusitis_llm_full_results.csv\"\n",
    "eval_df.to_csv(full_results_path, index=False)\n",
    "print(f\"Full results with all columns saved to '{full_results_path}'\")\n",
    "\n",
    "# Save a sample of 200 cases to a separate CSV for human evaluation.\n",
    "if len(eval_df) > 0:\n",
    "    sample_path = \"sinusitis_llm_human_review_sample.csv\"\n",
    "    sample_size = min(200, len(eval_df))\n",
    "    eval_df.head(sample_size).to_csv(sample_path, index=False)\n",
    "    print(f\"A sample of {sample_size} cases for human review saved to '{sample_path}'\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
